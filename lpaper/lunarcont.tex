\documentclass[conference]{IEEEtran}
\usepackage{graphicx,times,amsmath,algorithm,caption, subcaption,nicefrac,bm, url}
\usepackage{multicol}
\usepackage[noend]{algpseudocode}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{amssymb}


\pdfminorversion=3

\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}
\newcommand{\mathsc}[1]{{\normalfont\textsc{#1}}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\DeclareMathOperator*{\argmin}{arg\,min}
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
% % \usepackage[table]{xcolor}

\DeclareMathOperator{\dis}{d}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}

\newcommand{\twopartdef}[4]
{
	\left\{
		\begin{array}{ll}
			#1 & \mbox{if } #2 \\
			#3 & \mbox{otherwise } 
		\end{array}
	\right.
}

\IEEEoverridecommandlockouts

\begin{document}

% Paper title: keep the \ \\ \LARGE\bf in it to leave enough margin.
\title{\ \\ \LARGE\bf Rolling Horizon methods for Games with Continuous States and Actions } 
% TODO - better title

\author{Spyridon Samothrakis, Samuel A. Roberts, Diego Perez, Simon M. Lucas}

% Uncomment out the following line for invited papers
%\specialpapernotice{(Invited Paper)}

% Make the title area
\maketitle


\begin{abstract}
It is often the case that games have continuous dynamics and allow for continuous actions, possibly with with some added noise. For larger games with complicated dynamics, having agents learn offline behaviours in such a setting is a daunting task. On the other hand, provided a generative model is available, one might try to spread the cost of search/learning in a rolling horizon fashion (e.g. as in Monte Carlo Tree Search). In this paper we compare T-HOLOP (Truncated Hierarchical Open Loop Planning), an open loop planning algorithm at least partially inspired by MCTS, with a version of evolutionary planning that uses CMA-ES (which we call EVO-P) in two planning benchmark problems (Inverted Pendulum and the Double Integrator) and in Lunar Lander, a classic arcade game. We show that EVO-P outperforms T-HOLOP in the classic benchmarks, while T-HOLOP is unable to find a solution using the same heuristics. We conclude that off-the-shelf evolutionary algorithms can be used successfully in a rolling horizon setting, and that a different type of heuristics might be needed under different optimisation algorithms. 

\end{abstract}

% No keywords



\section{Introduction} \label{Introduction}
One of the most common uses of function optimisation is solving control problems. If an agent has access to a generative model of the world (i.e. a simulator) in which it is going to act, a control problem becomes a search/optimisation problem, usually referred to as ``simulation based planning''. Recently, a family of algorithms, mostly known as Monte Carlo Tree Search (MCTS)~\cite{browne2012survey}, has been applied to planning problems of discrete states and actions with considerable success. The focus of these algorithms is to attack extremely large state spaces of perfect information games, where one can sample rewards from the state space easily (e.g. Computer Go~\cite{gelly2006modification}). With access to a generative model (from which one can easily sample) and perfect sensor information, the following procedure works well: the agent receives sensor information, formulates a plan of action, performs the first action of that plan, receives a new state, re-formulates a plan, acts again (using the first action of its plan),  ad infinitum.

The most important reason for continuous re-planning is the fact that most planning algorithms' computational complexity is linear (or worse) in the number of states, whereas the number of states increases exponentially at each time step. One thus hopes to perform an action that looks good now, act, and replan, effectively creating a smaller problem or a ``closer to action'' horizon. This kind of behaviour is known as rolling horizon, sample based or model predictive control/planning~\cite{chang2007simulation}. Most of these algorithms are not exact; at each time step, due to the large state space, planning takes place in a Monte Carlo fashion, with random ``rollouts'' (i.e. simulations) guiding the algorithm in a best first search manner. All this planning and replanning is computationally intensive, thus very efficient use of samples should be made.  In the case of discrete state and actions, bandit algorithms~\cite{auer2002finite} provided a formidable solution to this problem.
An example of exploiting this efficient sampling can be found in Samothrakis et al.~\cite{samothrakis2011fast} where a high-performance MCTS Pac-Man agent
was developed using only $50 - 300$ simulations per action (the game simulator requested an action
from the controller every 40ms).


Recently, algorithms stemming from, or at least partially inspired by, MCTS aiming at solving planning problems involving continuous states and actions have come to light. A typical such algorithm is Hierarchical Optimistic Open Loop Planing (HOLOP)~\cite{weinstein2012bandit}. HOLOP is based partially on a strong~\cite{hoo2011} real-valued optimisation algorithm called HOO (for ``Hierarchical Optimistic Optimisation''). In this paper two contributions are made. The first contribution is the comparison of two versions of a very popular and strong evolutionary strategy: EVO-P, the open loop planning algorithm using Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES)~\cite{hansen2003reducing} and a version of HOLOP, which stands for Hierarchical Open Loop Optimistic Planning~\cite{hoo2011} on two standard benchmark problems, the double integrator and the reverse pendulum.  For speed purposes, we use T-HOO (Truncated Hierarchical Optimistic Optimisation), a faster version of HOO~\cite{hoo2011}, as the engine of HOLOP, hence we name the algorithm T-HOLOP.  Our second contribution is the use of these algorithms in Lunar Lander, a popular arcade game, coupled with a strong two-stage heuristic. We show that while EVO-P performs reasonably well, T-HOLOP struggles. 

While evolution is commonly used to evolve a reactive neural network controller (in a process commonly called neuro-evolution) for these and other RL problems,
it should be noted that evolution is being applied in a very different way here. In this paper, evolution is applied to perform each action given the current state.  This approach can only be applied when a generative
model is available, but has the advantage of offering immediate good performance without any
prior learning.  The disadvantage, compared to neuro-evolution, is that every action performed requires
CPU time for the simulations.  This is described in more detail in Section \ref{sec:Planning} and in line with what was done in Perez et al. \cite{perez2013rolling}, although this time in a continuous setting. 

The rest of this paper is structured as follows.  Section \ref{sec:Optimisation} discusses (briefly and epigrammatically) the relevant base algorithms that are going to be used in this paper, ((Truncated) Hierarchical Online Optimisation~\cite{hoo2011} and CMA-ES~\cite{hansen2003reducing}).
Section~\ref{sec:Planning} explains why and how the above algorithms can be used in the context of planning (and thus transformed into T-HOLOP and EVO-P respectively).  In Section \ref{sec:methodology} the experimental setup is described. In Section~\ref{sec:Experiments} a number of experiments are portrayed and analysed.  These form the bulk of this paper's contribution. We conclude with a short discussion in Section \ref{sec:conclusion}. 

\section{Algorithms for Continuous Function Optimisation} \label{sec:Optimisation}

This is the first background section of this paper and aims to introduce the two core optimisation algorithms on which our contributions are based. We will present both algorithms from an implementer's viewpoint, without explicitly justifying the design and mathematical choices behind them, nor going into too much detail. Interested readers should look at the cited bibliography for extensive discussions. 

In general, the optimisation problem can be defined as: Given a function $f:\bm{X}\rightarrow \bm{Y} $, where $\bm{X} \in \mathcal{R}^n$, with $n$ being the length of vector $\bm{X}$. The domain $\bm{X}$ is usually called the search space. Our goal is to find a vector $\bm{x}_0$, for which $f(\bm{x}) > f(\bm{x_0}) $ for all $x$ (also known as ``minimisation'') or $f(\bm{x}) < f(\bm{x_0})$ for all $x$ (known as ``maximisation'').
There are an enormous number of algorithms devoted to this problem, but only two will be presented and compared here. Both of these are considered state of the art in their respective fields and do not require the calculation of a gradient. If one has an analytic expression of the gradient or if the functions proposed are smooth enough, it might be better to look elsewhere for solutions (e.g. Conjugate Gradient Methods~\cite{shewchuk1994introduction}).

\subsection{CMA-ES} \label{sec:algCma}



The Covariance Matrix Adaptation - Evolutionary Strategy (CMA-ES) is an evolutionary algorithm designed for continuous domains, specially suited for non-linear and non-convex optimization problems~\cite{Hansen2006}. In general, this algorithm is applied to those problems that are not constrained and are made of up to $100$ dimensions.%~\footnote{\url{http://www.lri.fr/~hansen/cmatutorial-crop-crop.pdf}}

A multivariate normal distribution (MND) is a generalization of the univariate normal distribution, which is a probability distribution that has a bell-shaped probability density function (Gaussian function). A multivariate vector $X = (x_0, x_1, \dots x_N), x_i \in \mathcal{R}$ is said to have an MND if it satisfies that any linear combination from its components, $w_0x_0 + w_1x_1 + \dots + w_Nx_N$, is normally distributed.

CMA-ES creates a population of individuals by sampling from an MND: $\mathcal{N}(m,C)$, which is uniquely defined by the distribution mean $m \in \mathcal{R}^{n}$ and its covariance matrix $C \in \mathcal{R}^{n \times n}$. The top of the density function, which corresponds to $m$, also determines the translation of the distribution. The covariance matrix $C$, positive definite and symmetric, determines the shape of the distribution and its graphical interpretation: it defines an iso-density ellipsoid $\{ x_i \in \mathcal{R} | (x - m)^T C^{-1} (x - m) = 1\}$. Through the classic evolutionary setup, and by making clever use of population statistics, the algorithm proceeds from generation to generation of sampling until some convergence criteria has been reached. 



\subsection{T-HOO}
%$\bm{x^{C_l}_{\mathit{min}}} = \bm{x^{C}_{\mathit{min}}} $,
Recently, a family of algorithms based on discretizing the search domain has been proposed. Only one of these algorithms is going to be discussed here, Truncated Hierarchical Optimistic Optimisation (T-HOO)~\cite{hoo2011}. The T-HOO algorithm was built from the ground up to perform optimisation of noisy functions. It is an iterative, anytime algorithm, which is run for $n_0$ iterations.  The algorithm is based on a number of components. The first one is a data structure called a node and designated $\nu$. The node includes the maximum and minimum values of our search space, vectors $\bm{x_{\mathit{max}}}$ and $\bm{x_{\mathit{min}}}$ respectively. The node also has a property called $\mathit{count}$, which initially is set to zero and a property called $\mathit{sum}$, which, again, is initially set to zero. An unvisited node (i.e. $\mathit{count}=0$) is called a leaf node.  ``Splitting'' a node is the procedure of creating two children $C(\nu) = \{C_l(\nu),C_r(\nu)\}$. Both children are initialised $\bm{x_{\mathit{max}}}$ and $\bm{x_{\mathit{min}}}$ to their parents' node values when there are multiple dimensions to choose from, at $k$ according to some random distribution. Thus, for child $C_l(\nu)$ ,   $\bm{x^{C_l}_{\mathit{max,k}}} =\bm{x^{C}_{\mathit{min,k}}} +  \left(\bm{x^{C}_{\mathit{min,k}}} - \bm{x^{C}_{\mathit{max,k}}}\right)/2 $. For the other child,  $\bm{x^{C_r}_{\mathit{min,k}}} =  \bm{x^{C_l}_{\mathit{max,k}}}$. In other words, the father node is split into two children nodes, choosing which dimension to split uniform random (or according to some other criterion). Each node also has an associated value called $B(\nu)$, which is defined recursively as:  $B(\nu) = min \{U(\nu),max \{B(C_l(\nu)),B(C_r(\nu))\}\}$. Part of this equation is the term $U(\nu)$, which is defined as in Equation \ref{eq:U}:


\begin{equation}
U(\nu) = S(\nu)/N(\nu) + \sqrt{2ln n_0/N(\nu)} + \upsilon_1 \rho^h
\label{eq:U}
\end{equation}



If a dissimilarity metric is defined between different values of $\bm{X}$ as ${\lVert \bm{x}_1 - \bm{x}_2 \rVert}^\alpha$, one can set $\upsilon_1 =\sqrt{|A|}/2)^\alpha$ and $\rho = 2^{-\alpha/|A|}$. In our experiments $\alpha = 2$. If $N(\nu) = 0$, $B(\nu) = U(\nu) = \infty$, i.e. unvisited nodes have priority. If a node has not been visited then it is called a leaf node. If a node is a leaf node, one can ``draw'' a sample from it, by uniformly randomly sampling between $\bm{x_{\mathit{max}}}$ and $\bm{x_{\mathit{min}}}$.

The algorithm works by starting at the root node (the initial node). Since the leaf node and the root node coincide, HOOT samples from it and splits it. It can now proceed to the second iteration, where the algorithm follows a route using the maximum B-value of its children nodes, which it then samples and splits. The process repeats until the maximum number of iterations $n_0$ is over. If a certain depth is exceeded there is no point splitting nodes any more (given our iteration budget $n_0$). 



\section{Planning}  \label{sec:Planning}
In this section a formalised version of planing will be presented and an explanation provided on how the algorithms described in the previous section can be used to attack the problem.
\subsection{Markov Decision Processes}

The main decision theoretic abstraction for planning/control is the Markov Decision Process (MDP)~\cite{howard1960dynamic}. Formally,  an MDP is a tuple $\langle S,A,T,R,\gamma\rangle$, where:

\begin{itemize}
\item $S$ is a set of states, $s \in S$, with $s'$ being the next state in time. 
\item $A$ is a set of actions, each action named $a_j$.
\item  $T:S \times S \times  A  \rightarrow [0,1]$ is the probability of moving from state $s$ to state $s'$  after action $a \in A$ has taken place. $T(s'|s,a)$ denotes this probability.
\item  $R:S \rightarrow \Re$, $R(s)$ is a reward function at each state.
\item $\gamma$, a discount factor.
\end{itemize}

The MDP defines a single agent environment, fully observable to the agent. In an MDP, the Markov property holds (hence the name), which means that all the information an agent needs in order to act is embedded in the current state.
A possible route of action an agent might take is known as the \textit{policy} $\pi$. A policy is a probabilistic mapping between state and actions, $\pi:S \times A \rightarrow [0,1]$, thus $\pi(s,a) \in [0,1]$,  $\sum\limits_{a\in A}{\pi(s,a)} = 1$. The set of all policies is denoted as $\Pi$. The goal of an agent in an MDP environment is to maximise its long term value. Note that, since we are dealing with continuous actions and states here, sets $A$ and $S$ are metric (and not countable). Also note that we are NOT dealing with continuous time MDPs in this paper, although there is a notion of time. An example of the effects of the discount factor on rewards can be found in Figure \ref{fig:gamma}.



\begin{figure}[ht]
  \centering
  \includegraphics[width=0.45\textwidth]{graphics/gamma-crop.pdf}
  \caption{Examples of how rewards are affected by $\gamma$. Split probability reflects the probability of T-HOO splitting a node into two, while action depth reflects look-ahead depth}
  \label{fig:gamma}
\end{figure}


\subsection{Taxonomy of Planning Algorithms}
The terminology of Chang~\cite{chang2007simulation} is followed when trying to classify planning algorithms. If the policy $\pi:S \times A \rightarrow [0,1]$ is followed, what is known as ``closed loop'' planning takes place. The agent takes an action, senses its environment, takes another action etc. An alternative to closed-loop planning, ``open-loop'' planning, requires defining time. An agent sees a ``time'' when it's about to act instead of a state, i.e it doesn't have access to $s_t$, but just $t$. An open loop policy (or plan or control) is when an agent learns a policy with the form $\pi_o:A \times \mathcal{T} \rightarrow [0,1]$, where  $\mathcal{T}$ is an ordered set of time steps. Thus, an agent takes actions irrespective of the current state it is in. Intuitively this means that a sequence of actions is formed by the agent and the agent will take these actions in sequence (e.g. $(a_1,a_2,a_3,a_4,a_5,a_6,\dots,a_n)$). Obviously this is not optimal\footnote{Not optimal in the general case. It is optimal under the condition that the MDP is deterministic, i.e. each action leads to one specific state with probability one, and everything else has a probability zero}, although sometimes is much easier to do.


Another point of interest is \textit{when} planning happens. In open loop planning (and what is usually termed ``planning'' without any further qualifications), an agent forms a plan once and follows it until the end.

If the agent replans at every step, discarding or augmenting the plan received from previous steps, it is known as ``rolling horizon'' planning. The idea here is that, if the agent can plan as well as possible up to a  certain point, it can perform an action, move the planning horizon one step forward and replan. For example, Monte Carlo Tree Search for infinite MDPs can be seen as an approximate rolling horizon planning version of $TD(1)$\cite{silver2009reinforcement}.  The term ``Simulation based'' is used when the planning happens for an MDP that has a tree like structure and rewards are only visible at the end of the tree, a situation common in many games.

When learning how to act using a generative model (i.e. an internal simulator of feature events conditioned on actions), one can make a third distinction as to the type of sets represented by $A$ and $S$. In this paper the focus is on an $A$ and $S$ that come from a metric space. This means  that both sets' elements define a notion of distance and, for all practical purposes, have an infinite amount of elements. More specifically, both sets are drawn from a bounded set of real numbers, $\Re^n$.

Since an open loop policy is now a real-valued vector of actions, algorithms like T-HOO and CMA-ES can be used to find such a policy, hence the title of this paper. To the best of our knowledge this is the first time evolution has been used in such a setting. There are examples of closed loop rolling horizon papers, but these are beyond the scope of this article~\cite{xiang2012differential,samothrakis2010planning}.




\begin{algorithm}[!ht]
\caption{Rolling Horizon Open Loop Planning}\label{algo:ooop}
\begin{algorithmic}

\State 	\textbf{define external function} $\mathsc{FindOpenLoopPlan} (depth, T',S') $ \Comment{Try to find the optimal policy. You can use either evolution or T-HOO here}


	\Procedure{$\mathsc{OOP}$} {}
		\While{$!StoppingCondition$}
		    \State $a \gets \mathsc{FindOpenLoopPlan}(depth, T',S')[0]$ \Comment{Execute first action(s) of the policy - [0] gets the first action}
		    \State $s \sim T(s'|s,a)$ \Comment{Stop only at an absorbing state, sampling ($\sim$) from the environment}
		\EndWhile
		\State
	\EndProcedure



\end{algorithmic}
\end{algorithm}


\section{Methodology} \label{sec:methodology}
Our methodology is simple, yet efficient. We test T-HOLOP and EVO-P in two simple benchmark problems (see below for more detail) and in Lunar Lander, a classic arcade game. EVO-P is simply CMA-ES used in a rolling horizon setting, while T-HOLOP is T-HOO used in the same manner.  


\subsection{Simple Planning Benchmarking Problems}

% % \todo[inline]{Simon to do this}.




For direct comparison with recent work on sample based planning using trees, in particular
Weinstein and Littman~\cite{weinstein2012bandit} and Pazis and Lagoudakis \cite{PazisBAS},
we used the same problems: double integrator and inverted pendulum.  The basic
experimental setup followed Weinstein and Littman~\cite{weinstein2012bandit}
and Pazis and Lagoudakis \cite{PazisBAS}
closely, except that they used a fixed noise level for each experiment whereas we varied the
noise level to explore how performance was affected.

Both problems are modelled
using continuous state discrete time simulations.  In each case the desired
acceleration is corrupted by additive uniform random noise
before being limited within the specified range.  The noise levels are described in the next section.


\subsubsection{Double Integrator}

The double integrator problem is to control a mass along a single dimension.  The state space
is 2-dimensional, consisting of $(p,v)$ where $p$ is position and $v$ is velocity.
The goal is to change the state from $(1,0)$ to $(0,0)$.  Both position and velocity
are clamped to be within the range $-2$ to $+2$.  At each time step the controller selects
the desired acceleration $a'$, which then is noise corrupted and range limited between $-1.5N$ to $+1.5$N to yield the
applied acceleration $a$.

Euler integration is then used to update the position and velocity, given the time step $\delta_t$:

\begin{eqnarray}
v_{t+\delta_t} & = & v_{t} + a \delta_t \\
p_{t+\delta_t} & = & p_{t} + v_{t+\delta_t} \delta_t
\end{eqnarray}

At each time step the reward $r$ is:
\begin{equation}
 r = -(p^2 + a^2)
\end{equation}

The problem becomes harder as the time interval $\delta_t$ that is applied at each action, increases,
and following Pazis and Lagoudakis we set $(\delta_t = 0.5s)$.

% \begin{equation}
% R = -(p^2 + a^2)
% \end{equation}

\subsubsection{Inverted Pendulum}

The inverted pendulum problem is also known as the pole balancing
or cart-pole problem.  The goal is to keep the pendulum / pole as upright and as still as possible.
This is achieved by accelerating the cart which affects the freely pivoted pendulum.

In this instance of the problem the controller only sees a two dimensional
state space, consisting of the
angle $\theta$ that the inverted pendulum deviates from the vertical, and the angular velocity
of the pendulum $\dot{\theta}$.
The controller selects the desired angular force $F$,
which is then corrupted by additive uniform random noise and limited in the range $-50N$ to $+50$N
so as to yield the applied angular force $u$.  The angular acceleration $\ddot{\theta}$ is then calculated
as follows:

\begin{equation}
\ddot{\theta} = \frac{g sin(\theta) - \alpha m l (\dot{\theta})^2 sin(2 \theta) / 2 - \alpha cos(\theta) u}
                     {4l/3 - \alpha m l  cos^2(\theta)}
\end{equation}

where $g$ is the acceleration due to gravity
$(g = 9.8ms^{-2})$,
$m$ is the mass of the pendulum
$(m = 2kg)$, $(\alpha = 1 / (m + M))$, $M$ is the mass of the cart
$(M = 8kg)$ and $l$ is the length of the pendulum $(l = 0.5m)$.
The time interval $\delta_t$ was set to $0.1s$: i.e.\ the simulation is
updated 10 times per second:

\begin{eqnarray}
\dot{\theta}_{t+\delta_t} & = &  \dot{\theta}_{t} + \ddot{\theta} \delta_t \\
\theta_{t+\delta_t}  & = & \theta_{t} + \dot{\theta}_{t+\delta_t} \delta_t
\end{eqnarray}

At each timestep the reward $r$ punishes deviation from the vertical, high speed, and high force:
\begin{equation}
r = -((2 \theta / \pi)^2 + \dot{\theta}^2 + (F / 50)^2 )
\label{eq:rew:t}
\end{equation}


\subsection{Lunar Lander}
In this section we give a brief overview of a problem based upon {\itshape Lunar Lander}, a popular arcade game. The goal in {\itshape Lunar Lander} is to land a spaceship on a flat plateau on an otherwise jagged landscape. In order to do this, the player must use a supply of fuel to apply thrust to a spaceship, which has inertia. The ship is able to rotate at no cost to fuel usage, but must contend with rotational inertia.

\subsubsection{Environmental Properties}

The properties of the base environment of {\itshape Lunar Lander} are that it is frictionless, and that it is a two-dimensional plane with horizontal wrapping. This can also be conceptualised as a cylinder. Anything that passes from the left edge of the playing field moves to the right instantaneously, and vice versa. The landscape is constructed as a series of line segments, with each vertex being distributed equally horizontally, and randomly vertically.

\subsubsection{Spaceship Physics}

The spaceship within this game world is modelled as a circular mass with some physical properties, such as a position within the game world $\bm {s}$, a velocity $\bm {v}$, an orientation $\bm{d}$, an angular velocity $\omega$ and a radius of a bounding sphere $r$ used for collision detection with the landscape.

This collision detection method is based on the spaceship's bounding circle overlapping the corresponding point on the landscape on the same vertical axis, as can be seen in Figure \ref{fig:diagram_landscapepoint}.


\begin{figure}[hbtp]
\centering
\includegraphics[scale=0.35]{graphics/landscapepoint}
\caption{The point closest to the spaceship on the landscape lies in between two of the defined vertices of the landscape, and requires interpolation to calculate the y co-ordinate from the x co-ordinate of the ship.}
\label{fig:diagram_landscapepoint}
\end{figure}

For the ship's centre $\bm {s}$, the left nearest landscape vertex $\bm {p}^{l}$ and the right nearest landscape vertex $\bm {p}^{r}$, the point of collision $\bm {p}^{c}$ against the landscape is calculated as 




\begin{figure*}[ht]
  %\centering



  \centering
        \begin{subfigure}[b]{0.32\textwidth}
                \centering
                \includegraphics[width=1.0\textwidth]{graphics/online-IP-crop.pdf}
                \caption{Scores for a noise level of $l=1$ under variable function evaluations. }
                \label{fig:IP}
        \end{subfigure}   \begin{subfigure}[b]{0.32\textwidth}
                \centering
                \includegraphics[width=1.0\textwidth]{graphics/online-noise-IP_50-crop.pdf}
                \caption{Scores for 50 iterations under variable noise level $l$.}
                \label{fig:IP-N-50}
        \end{subfigure}  \begin{subfigure}[b]{0.32\textwidth}
                \centering
                \includegraphics[width=1.0\textwidth]{graphics/online-noise-IP_100-crop.pdf}
                \caption{Score for 100 iterations under variable noise level $l$.}
                \label{fig:IP-N-100}
        \end{subfigure}%

         \caption{Performance of both T-HOLOP and EVO-P in the Inverted Pendulum benchmark (error bars for the 95th percentile). Higher scores are better.}
         \label{fig:IP-all}

\end{figure*}


        \begin{figure*}[ht]

        \begin{subfigure}[b]{0.32\textwidth}
                \centering
                \includegraphics[width=1.0\textwidth]{graphics/online-DI-crop.pdf}
                \caption{Scores for a noise level of $l=1$ under variable function evaluations.}
                \label{fig:DI}
        \end{subfigure}   \begin{subfigure}[b]{0.32\textwidth}
                \centering
                \includegraphics[width=1.0\textwidth]{graphics/online-noise-DI_50-crop.pdf}
                  \caption{Scores for 50 iterations under variable noise level $l$. }
                \label{fig:DI-N-50}
        \end{subfigure}   \begin{subfigure}[b]{0.32\textwidth}
                \centering
                \includegraphics[width=1.0\textwidth]{graphics/online-noise-DI_100-crop.pdf}
                \caption{Score for 100 iterations an variable noise level $l$.}
                \label{fig:DI-N-100}
        \end{subfigure}

  \label{fig:DI-all}

       \caption{Performance of all algorithms in the Double Integrator benchmark (error bars for the 95th percentile). Higher scores are better.}


\end{figure*}

\begin{equation}
\bm {p}_{x}^{c} = \bm {s}_{x}
\bm {p}_{y}^{c} = \bm {p}_{y}^{l} + v(\bm {p}_{y}^{r} - \bm {p}_{y}^{l})
\end{equation}

where $v$ is a value between $0$ to $1$ used for interpolation, and can be calculated as follows.

\begin{equation}
v = \frac{ {\bm {s}_{x} - \bm {p}_{x}^{l}}}{ {\bm {p}_{x}^{r} - \bm {p}_{x}^{l}} }
\end{equation}

Collision is then true if the following statement is true:

\begin{equation}
\bm {s}_{y} + r \geq \bm {p}^{c}_{y}
\end{equation}

The ship colliding with the landscape constitutes the end of the {\itshape Lunar Lander} game. The conditions surrounding this collision, including speed and orientation of the ship, constitute whether the nature of the collision is a success or a failure. In this experimental setup we accept a velocity of $5$ and an angle of $< 0.1$ radians as successful landing criteria.

An agent can modify the ship's direction by applying linear thrust $\bm{a_T}$ and angular impulse $\bm{a_I}$. The controllers have to identify a vector of real numbers $\bm{A}$, with each element of the set $a_i \in [0,1]$.  Each even element of the vector represents the linear thrust, whereas each odd element represents angular velocity, actions that the agent can take. Each linear thrust vector element is transformed by applying the following: 

\begin{equation}
\bm{a_I} \gets 5(\bm{a_I} - 0.5)
\bm{a_T} \gets 300\bm{a_T} 
\end{equation}

Finally each linear thrust and angular impulse is repeated for $X = 7$ actions, the effects of which we are going to explore below. 

\subsubsection{Heuristic} % this too

Lunar Lander has a setup where an agent gets rewards if and only if it manages to actually land the ship on the pad. This creates the problem that rollouts (whether used by EVO-P or T-HOLOP) will rarely see a reward. In order to circumvent this, as it is a problem common to other games as well, we use a heuristic for each state. The heuristic was partially inspired by the two-stage approach found here : \url{http://www.cs.berkeley.edu/~jrs/4/lunar.html}\cite{lunar}. Initially, we need to define a number of vectors that portray ship directions: 

\begin{align}
& \bm{du} = [1,0]\notag\\ 
& \bm{dl} = [1,2]\notag\\
& \bm{dr} = [1,-2]
\label{eq:vectors}
\end{align}


The vectors defined in Equation \ref{eq:vectors} define three directions for the ship. The first one, $\bm{du}$, defines a ship standing upright, the second one, $\bm{dl}$, defines it tilting to the left, while the third one $\bm{dr}$ tilting to the right. We make use of these vectors in order to help align the ship in the right direction, depending on where the landing point is. 


\begin{align}
f &= \twopartdef{-\sum{arccos(\bm{dl}\cdot\bm{d}/(\norm{\bm{dl}}\norm{\bm{d}}}))}{\bm{np_x} < \bm{s_x}}{-\sum{arccos(\bm{dr}\cdot\bm{d}/(\norm{\bm{dr}}\norm{\bm{d}}}))} {otherwise}\notag\\
l &= -\sum{arccos(\bm{du}\cdot\bm{d}/(\norm{\bm{du}}\norm{\bm{d}}})) - \abs{\bm {v_y} - V_{y}^{max}}\notag\\
h &= \twopartdef{ \abs{-(\norm{\bm {v}} - C) - \abs{\bm {s}_{x} - \bm {np}_{x}}} - f}{\norm{\bm {s} - \bm {np}} < D}
{l - \abs{\bm {v_x} -C_l }  }{Otherwise}\notag\\
h&\gets h + p 
\label{eq:h1}
\end{align}

In the above Equation \ref{eq:h1} the constant C denotes the maximum magnitude of speed, which we set to $C=30$. $D = 60$ is the distance from the landing point $np$, where this heuristic takes precedence.  Intuitively, the upper part of the heuristic $h$ tries to get the ship above the landing point as fast as possible while maintaining a certain speed. The second part of the heuristic tries to minimise velocity on the $x$ axis (as the ship tries to land with maximum speed $C_l = 2$), maintain a minimum landing speed on the $y$ axis and get a certain angle (the summation term $f$) as close to vector $du$ as possible, i.e. help the ship stay upright through the duration of the rollout.  It's worth noting that the main reason why we use a heuristic of this kind (one that includes not only the final state, but makes use of the path of the state as well) is that there is not enough information to discern states if one only takes into account ship direction.  Without incorporating the angular velocity,  the ship might be spinning madly, but the end of the rollout might find it in the correct direction. Finally, notice that the heuristic $h$ gets the value $p=10000$ if the ship has crashed at any point during the rollout.   

\section{Results} \label{sec:Experiments}
We present the results of our experiments below.




% \begin{figure*}[ht]
%   %\centering



%   \centering
%         \begin{subfigure}{0.49\textwidth}
%                 \centering
%                 \includegraphics[width=1.0\textwidth]{./lunarexps/sample-crop.pdf}
%                 \caption{Best value for each tick EVO-P could find. }
%                 \label{fig:sample}
%         \end{subfigure}  
%         \begin{subfigure}{0.49\textwidth}
%                 \centering
%                 \includegraphics[width=1.0\textwidth]{./lunarexps/sample-detail-crop.pdf}
%                 \caption{Detail of Figure \ref{fig:sample}, when the ship is closer to landing.}
%                 \label{fig:sample-detail}

%         \end{subfigure}
       

%          \caption{Sample run for EVO-P. Best fitness discovered at each tick is shown. Notice the big ``bump'' in fitness when the ship gets close to the landing pad}
%          \label{fig:samples}

% \end{figure*}


\begin{figure*}[ht]
  \centering



  \centering
        \begin{subfigure}{0.49\textwidth}
                \centering
                \includegraphics[width=1.0\textwidth]{./graphics/map2.pdf}
                \caption{Map A,  where the ship is lower than the pad.}
                \label{fig:map2}
        \end{subfigure}  
        \begin{subfigure}{0.49\textwidth}
                \centering
                \includegraphics[width=1.0\textwidth]{./graphics/map1-easy.pdf}
                \caption{Map B, an easy map where only the first heuristic is used.}
                \label{fig:map1-easy}

        \end{subfigure}
\\
        \begin{subfigure}{0.49\textwidth}
                \centering
                \includegraphics[width=1.0\textwidth]{./graphics/map1-tall.pdf}
                \caption{Map C, where the ship is close to the top of the screen.}
                \label{fig:map1-tall}\end{subfigure}\begin{subfigure}{0.49\textwidth}
                \centering
                \includegraphics[width=1.0\textwidth]{./graphics/map1-wall.pdf}
                \caption{Map D, where the ship is behind a wall.}
                \label{fig:map-wall}

        \end{subfigure}
       

         \caption{The four different maps used in our experiments. The maps capture different position/properties.}
         \label{fig:maps}

\end{figure*}

\subsection{Benchmark Problems}


For the second set of experiments we have defined the following variables. The first two involve running the experiments with a fixed rollout depth of 50, i.e. each algorithm at each step plans for 50 steps ahead. We set the noise levels to $0.1 (\mathcal{U}(-0.5,0.5)) l$ uniform noise for the double integrator experiment and $10 (\mathcal{U}(-0.5,0.5)) l$  for the Inverted Pendulum (different noise levels for different problems come from the fact that the problems have unequal features).  The variable $l$ is the noise level which defines how noisy the environment is. Initially, we performed two experiments, with a variable number of iterations and a fixed noise level of one. In all experiments the default CMA implementation is used~\footnote{\url{https://www.lri.fr/~hansen/cmaes_java.tar.gz}}. 


The results of these experiments can be seen in graphs  \ref{fig:IP} and \ref{fig:DI}. These are close to the original experiments of Weinstein and Littman~\cite{weinstein2012bandit}, only this time we varied the number
of iterations allowed for each algorithm. The original experiment had given the algorithms 200 value iterations. With this number of iterations, for the Inverted Pendulum experiment the mean reported (using HOLOP) was $-47.45$. We got a better mean using T-HOLOP
% % \footnote{We believe there is a minor bug in the HOLOP implementation Weinstein et al use.}
\footnote{This was due to a minor difference between our implementation of HOLOP and Weinstein's.}
 of $-42.94$.

 EVO-P got a score of $-41.69$. It also seems to experience a drop in performance as we increase function evaluations, presumably because it converges to a false minima without any uncertainty handling mechanism to stop this. This kind of behaviour has been observed previously in Evolutionary Algorithms~\cite{hansen2009tec} when having noisy function.

For the double integrator experiment, we had slightly different results. The Weinstein paper reports a mean of $-2.72$ for 200 iterations. Using T-HOLOP we got a better mean of $-2.62$. A far better result of $-1.74$ was obtained by EVO-P. 


Results for the variable noise levels $l$ (Figures \ref{fig:IP-N-50}, \ref{fig:IP-N-100}, \ref{fig:DI-N-50}, \ref{fig:DI-N-100}) closely mimic the performance of their constant noise counterparts. For the inverted pendulum, using just 50 iterations (Figure \ref{fig:IP-N-50}) EVO-P is able to stabilise the pendulum with $50$ iterations. This still applies if we increase the number of function evaluations to 100 (Figure \ref{fig:DI-N-100}). On the other hand, the results start to get closer to the fixed noise level experiments if we increase the number of iterations in the Inverted Pendulum experiment to 100 (Figure \ref{fig:IP-N-50}).

\subsection{Lunar Lander}

\begin{table}[h]
	\begin{tabular}{lrrrr}
\hline
 Map Name   &   Success Mean &   Success CI &   Ticks Mean &   Tick CI \\
\hline
 Map A       &            1   &     0        &       2925.9 &   $\pm 184.437$ \\
 Map B  &            1   &     0        &       2733   &   $\pm 159.34$  \\
 Map C  &            1   &     0        &       2293   &   $\pm 123.917$ \\
 Map D  &            0.9 &     $\pm 0.226216$ &       2421.6 &   $\pm 682.893$ \\
\hline
\end{tabular}
\label{tb:results}
\caption{EVO-P results for different Lunar Lander maps.}
\end{table}


For Lunar Lander, we perform $10$ runs for each map in Figure \ref{fig:maps}, and the results for EVO-P can be seen in Table I. The results include the mean number of successful landings (\emph{Success Mean}) and the mean time it took to land the ship (\emph{Ticks Mean}), alongside the confidence for the $95$th interval.   For EVO-P we used the default CMA-ES population size, each individual gene bound between $[0,1]$, each gene had a length of $8$ and each action pair ($x[i], x[i+1]$) was repeated $7$ times (where $x[i]$ represents thrust and $x[i+1]$ angular thrust). We derived these numbers experimentally and did not optimise for them (not even implicitly) to any serious degree.  

There are a number of things to notice here. The first is that only in Map D, where the ship starts the problem facing a wall, do we get a failure to land, and this is due to a crash. This has to do with the fact that even a minor thrust can push you a bit forward and drive the ship to a position where there is no longer the possibility of recovery from a crush (or the combination of that recovery is extremely hard to find). The second thing to observe is that we get there slowly, but this is by design, as we he have made the flying speed of the ship controllable. One can potentially tweak all the default parameters we have used, plus the ones incorporated in CMA-ES, as a kind of meta-heuristic search and achieve much better results. Another thing to note is that EVO-P lands the ship (on average) almost at the same time on MAPS B, C, D, although map B has the same landscape as the other two but the ship is much closer. This is due to the fact that the minimum landing speed, set on one of our heuristics, is really low. The final thing we would like to point out is that we have only included the results for EVO-P because T-HOLOP failed to land the ship using the same heuristic, and behaved badly. It's worth noting here that T-HOLOP requires rewards between certain bounds, so they can be normalised around $[0,1]$ and this is not the case with our heuristic. For completeness purposes, we did change the UCB-like node selection policy of T-HOO to e-greedy, in which case we \emph{did} get the ship to behave reasonably, albeit the failure to land was evident here as well. This, we think, is important for a number of reasons. The first one is that it shows that different Heuristics might be needed for different algorithms. Although this might be self-evident, it shows that one must design heuristics with a specific algorithm in mind. The second is, but we can speculate only at this time, that T-HOLOP spends too much effort trying to solve a hard problem than the one at hand. It will converge eventually, but the the Gaussian approximation of CMA-ES seems to outperform it in the short term. 


\section{Conclusion} \label{sec:conclusion}
We showed that it is doable to use evolution in a rolling horizon setting, making it possible to create agents using off-the-shelf evolutionary algorithms and a generative model. We used an experimental setup that included both simple benchmark problems and a popular arcade game, Lunar Lander. Unfortunately, it seems that different algorithms require different heuristics to work properly in the case that absorbing states are not met during rollouts. Definite conclusions are hard to draw about the quality and/or the suitability of each method. There are a multitude of hyperparameters to optimise and each method responds better to different heuristics. On top of that, implementations of each method might have different memory/speed requirements on different platforms - which makes direct comparison even harder. If anything, such intricacies require more specific set of experiments and a conscious effort to understand which method behaves better under what conditions. Overall however we think it might be possible to incorporate simplifying assumptions into T-HOLOP priors, thus making it easier to search harder distributions.  In all cases however, one might want to include sensible defaults in all algorithms, and use them without too much hassle; CMA-ES is probably as close to that as one can imagine for continuous settings, so it might be worth considering it as a ``first option'' in most settings (including the one we use here). 
In future research we aim to improve MCTS-like algorithms for continuous settings by including a learning module, possibly in the form of actor-critic algorithms. This should automate learning heuristics (after some training games) and help guide the rollouts towards more sensible solutions, even in the case where heuristics do not favour T-HOLOP. 

 
% Use \section* for the acknowledgments
\section*{Acknowledgements}
This work was supported by EPSRC grant EP/H048588/1 entitled: ``UCT for Games and Beyond'' and an EPSRC Ph.D studentship. 



\bibliographystyle{IEEEtran}
\bibliography{lunarcontbib}
\end{document}=